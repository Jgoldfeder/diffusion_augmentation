{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pat/miniconda3/envs/diff_aug2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ControlLDM: Running in eps-prediction mode\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "making attention of type 'vanilla-xformers' with 512 in_channels\n",
      "building MemoryEfficientAttnBlock with 512 in_channels...\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla-xformers' with 512 in_channels\n",
      "building MemoryEfficientAttnBlock with 512 in_channels...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pat/miniconda3/envs/diff_aug2/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Loaded model config from [./models/control_v11p_sd15_canny.yaml]\n",
      "Loaded state_dict from [./models/v1-5-pruned.ckpt]\n",
      "Loaded state_dict from [./models/control_v11p_sd15_canny.pth]\n",
      "ControlLDM: Running in eps-prediction mode\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "making attention of type 'vanilla-xformers' with 512 in_channels\n",
      "building MemoryEfficientAttnBlock with 512 in_channels...\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla-xformers' with 512 in_channels\n",
      "building MemoryEfficientAttnBlock with 512 in_channels...\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Loaded model config from [./models/control_v11f1p_sd15_depth.yaml]\n",
      "Loaded state_dict from [./models/v1-5-pruned.ckpt]\n",
      "Loaded state_dict from [./models/control_v11f1p_sd15_depth.pth]\n",
      "ControlLDM: Running in eps-prediction mode\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "making attention of type 'vanilla-xformers' with 512 in_channels\n",
      "building MemoryEfficientAttnBlock with 512 in_channels...\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla-xformers' with 512 in_channels\n",
      "building MemoryEfficientAttnBlock with 512 in_channels...\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Loaded model config from [./models/control_v11p_sd15_seg.yaml]\n",
      "Loaded state_dict from [./models/v1-5-pruned.ckpt]\n",
      "Loaded state_dict from [./models/control_v11p_sd15_seg.pth]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import random\n",
    "import einops\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration, SamModel, AutoImageProcessor, DPTForDepthEstimation\n",
    "from transformers import pipeline\n",
    "from annotator.util import resize_image, HWC3\n",
    "from annotator.canny import CannyDetector\n",
    "from annotator.uniformer import UniformerDetector\n",
    "from annotator.midas import MidasDetector\n",
    "\n",
    "from cldm.model import create_model, load_state_dict\n",
    "from cldm.ddim_hacked import DDIMSampler\n",
    "from pytorch_lightning import seed_everything\n",
    "\n",
    "\n",
    "# Initialize ControlNet models\n",
    "model_names = ['control_v11p_sd15_canny', 'control_v11f1p_sd15_depth', 'control_v11p_sd15_seg']\n",
    "models = {}\n",
    "for name in model_names:\n",
    "    model = create_model(f'./models/{name}.yaml').cpu()\n",
    "    model.load_state_dict(load_state_dict('./models/v1-5-pruned.ckpt', location='cuda'), strict=False)\n",
    "    model.load_state_dict(load_state_dict(f'./models/{name}.pth', location='cuda'), strict=False)\n",
    "    models[name] = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pat/miniconda3/envs/diff_aug2/lib/python3.10/site-packages/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name vit_base_resnet50_384 to current vit_base_r50_s16_384.orig_in21k_ft_in1k.\n",
      "  model = create_fn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use Checkpoint: False\n",
      "Checkpoint Number: [0, 0, 0, 0]\n",
      "Use global window for all blocks in stage3\n",
      "load checkpoint from local path: /home/pat/diffusion_augmentation/annotator/ckpts/upernet_global_small.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/pat/miniconda3/envs/diff_aug2/lib/python3.10/site-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.12s/it]\n"
     ]
    }
   ],
   "source": [
    "# Initialize detectors\n",
    "\n",
    "apply_canny = CannyDetector()\n",
    "apply_depth = MidasDetector()\n",
    "apply_seg = UniformerDetector()\n",
    "\n",
    "# Initialize LLAVA model\n",
    "model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "llava_model = LlavaForConditionalGeneration.from_pretrained(model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(det, input_image, prompt, a_prompt, n_prompt, num_samples, image_resolution, detect_resolution, ddim_steps, guess_mode, strength, scale, seed, eta, low_threshold, high_threshold):\n",
    "    with torch.no_grad():\n",
    "        # input_image is pil image need to change back to numpy\n",
    "        input_image = np.array(input_image)\n",
    "        input_image = HWC3(input_image)\n",
    "        input_image = resize_image(input_image, detect_resolution)\n",
    "        H, W, C = input_image.shape\n",
    "        if det == 'Canny':\n",
    "           \n",
    "            detected_map = apply_canny(input_image, low_threshold, high_threshold)\n",
    "            detected_map = HWC3(detected_map)\n",
    "            detected_map = cv2.resize(detected_map, (W, H), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "            # Choose model and sampler\n",
    "            model = models['control_v11p_sd15_canny']\n",
    "            ddim_sampler = DDIMSampler(models['control_v11p_sd15_canny'])  # Using Canny model for sampling\n",
    "\n",
    "        elif det == 'Depth':\n",
    "            detected_map, _ = apply_depth(input_image)\n",
    "            detected_map = HWC3(detected_map)\n",
    "            detected_map = cv2.resize(detected_map, (W, H), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "            depth = Image.fromarray(detected_map)\n",
    "            depth.save(f\"./test/preprocessed_depth.png\")\n",
    "            # print(models.keys())\n",
    "\n",
    "            # Choose model and sampler\n",
    "            model = models['control_v11f1p_sd15_depth']\n",
    "            ddim_sampler = DDIMSampler(models['control_v11f1p_sd15_depth'])  # Using Depth Anything model for sampling\n",
    "\n",
    "\n",
    "        elif det == 'Segmentation':\n",
    "            detected_map = apply_seg(input_image)\n",
    "            detected_map = cv2.resize(detected_map, (W, H), interpolation=cv2.INTER_NEAREST)\n",
    "            # print(type(detected_map))\n",
    "            # show_masks_on_image(input_image, masks)\n",
    "            Image.fromarray(detected_map).save(f\"./test/preprocessed_segment.png\")\n",
    "\n",
    "            # Choose model and sampler\n",
    "            model = models['control_v11p_sd15_seg']\n",
    "            ddim_sampler = DDIMSampler(models['control_v11p_sd15_seg'])\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown detection type: {det}\")\n",
    "\n",
    "        img = resize_image(HWC3(np.array(input_image)), image_resolution)\n",
    "        H, W, C = img.shape\n",
    "\n",
    "        \n",
    "\n",
    "        control = torch.from_numpy(detected_map.copy()).float().cuda() / 255.0\n",
    "        control = torch.stack([control for _ in range(num_samples)], dim=0)\n",
    "        control = einops.rearrange(control, 'b h w c -> b c h w').clone()\n",
    "\n",
    "        if seed == -1:\n",
    "            seed = random.randint(0, 65535)\n",
    "        seed_everything(seed)\n",
    "\n",
    "        model.low_vram_shift(is_diffusing=False)\n",
    "\n",
    "        cond = {\"c_concat\": [control], \"c_crossattn\": [model.get_learned_conditioning([prompt + ', ' + a_prompt] * num_samples)]}\n",
    "        un_cond = {\"c_concat\": None if guess_mode else [control], \"c_crossattn\": [model.get_learned_conditioning([n_prompt] * num_samples)]}\n",
    "        shape = (4, H // 8, W // 8)\n",
    "\n",
    "        model.low_vram_shift(is_diffusing=True)\n",
    "\n",
    "        model.control_scales = [strength * (0.825 ** float(12 - i)) for i in range(13)] if guess_mode else ([strength] * 13)\n",
    "\n",
    "        samples, intermediates = ddim_sampler.sample(ddim_steps, num_samples,\n",
    "                                                     shape, cond, verbose=False, eta=eta,\n",
    "                                                     unconditional_guidance_scale=scale,\n",
    "                                                     unconditional_conditioning=un_cond)\n",
    "\n",
    "        model.low_vram_shift(is_diffusing=False)\n",
    "\n",
    "        x_samples = model.decode_first_stage(samples)\n",
    "        x_samples = (einops.rearrange(x_samples, 'b c h w -> b h w c') * 127.5 + 127.5).cpu().numpy().clip(0, 255).astype(np.uint8)\n",
    "\n",
    "        results = [x_samples[i] for i in range(num_samples)]\n",
    "        #save the result\n",
    "        Image.fromarray(results[0]).save(f\"./test/augmented_{det}.png\")\n",
    "    return detected_map, results[0]  # Return both the preprocessed map and the augmented image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(image, class_label, det_type):\n",
    "\n",
    "    # Step 1: Generate caption using LLAVA\n",
    "    pil_image = to_pil_image(image) if not isinstance(image, Image.Image) else image\n",
    "    # prompt = \"USER: <image>\\nDescribe the image in detail in as many words as possible. Describe the colors, what's in focus, out of focus, on the ground, and in the sky. \\nASSISTANT:\"\n",
    "    # inputs = processor(prompt, images=[pil_image], return_tensors=\"pt\")\n",
    "    \n",
    "    # generated_ids = llava_model.generate(**inputs, max_length=100)\n",
    "    # caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "    caption = \"a tent in the middle of a field\"\n",
    "    print(f\"Generated caption: {caption}\")\n",
    "    \n",
    "    \n",
    "    # Step 3: Apply ControlNet with different detection types\n",
    "    augmented_images = {}\n",
    "    preprocessed_images = {}\n",
    "    \n",
    "    print(f\"Processing {det_type}...\")\n",
    "    preprocessed, augmented = process(\n",
    "        det=det_type,\n",
    "        input_image=pil_image,\n",
    "        prompt=caption,\n",
    "        a_prompt=\"clear image, photorealistic\",\n",
    "        n_prompt=\"multiple, mushed, low quality, cropped, worst quality\",\n",
    "        num_samples=1,\n",
    "        image_resolution=512,\n",
    "        detect_resolution=512,\n",
    "        ddim_steps=20,\n",
    "        guess_mode=False,\n",
    "        strength=1.0,\n",
    "        scale=7.5,\n",
    "        seed=-1,\n",
    "        eta=0.0,\n",
    "        low_threshold=100,\n",
    "        high_threshold=200\n",
    "    )\n",
    "    augmented_images[det_type] = Image.fromarray(augmented)\n",
    "    preprocessed_images[det_type] = Image.fromarray(preprocessed)\n",
    "\n",
    "    \n",
    "    return image, preprocessed_images, augmented_images, class_label, caption\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_image = Image.open(\"/home/pat/diffusion_augmentation/test/original.png\")\n",
    "example_image.save(\"./test/original.png\")\n",
    "example_class = \"tent\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated caption: a tent in the middle of a field\n",
      "Processing Canny...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 34497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is (1, 4, 64, 96), eta 0.0\n",
      "Running DDIM Sampling with 20 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 20/20 [00:03<00:00,  5.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tent\n",
      "Caption: a tent in the middle of a field\n"
     ]
    }
   ],
   "source": [
    "det_type = 'Canny'\n",
    "\n",
    "original, preprocessed_dict, augmented_dict, label, caption = augment(example_image, example_class, det_type)\n",
    "\n",
    "preprocessed_dict[det_type].save(f\"./test/preprocessed_{det_type.lower()}.png\")\n",
    "augmented_dict[det_type].save(f\"./test/augmented_{det_type.lower()}.png\")\n",
    "print(f\"Class: {label}\")\n",
    "print(f\"Caption: {caption}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated caption: a tent in the middle of a field\n",
      "Processing Depth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 53308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is (1, 4, 64, 96), eta 0.0\n",
      "Running DDIM Sampling with 20 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 20/20 [00:03<00:00,  6.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tent\n",
      "Caption: a tent in the middle of a field\n"
     ]
    }
   ],
   "source": [
    "det_type = 'Depth'\n",
    "original, preprocessed_dict, augmented_dict, label, caption = augment(example_image, example_class, det_type)\n",
    "\n",
    "preprocessed_dict[det_type].save(f\"./test/preprocessed_{det_type.lower()}.png\")\n",
    "augmented_dict[det_type].save(f\"./test/augmented_{det_type.lower()}.png\")\n",
    "print(f\"Class: {label}\")\n",
    "print(f\"Caption: {caption}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated caption: a tent in the middle of a field\n",
      "Processing Segmentation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pat/diffusion_augmentation/annotator/uniformer/mmseg/models/segmentors/base.py:271: UserWarning: show==False and out_file is not specified, only result image will be returned\n",
      "  warnings.warn('show==False and out_file is not specified, only '\n",
      "Global seed set to 43986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is (1, 4, 64, 96), eta 0.0\n",
      "Running DDIM Sampling with 20 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 20/20 [00:03<00:00,  6.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: tent\n",
      "Caption: a tent in the middle of a field\n"
     ]
    }
   ],
   "source": [
    "det_type = 'Segmentation'\n",
    "\n",
    "original, preprocessed_dict, augmented_dict, label, caption = augment(example_image, example_class, det_type)\n",
    "\n",
    "preprocessed_dict[det_type].save(f\"./test/preprocessed_{det_type.lower()}.png\")\n",
    "augmented_dict[det_type].save(f\"./test/augmented_{det_type.lower()}.png\")\n",
    "print(f\"Class: {label}\")\n",
    "print(f\"Caption: {caption}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import colorsys\n",
    "def random_color_augmentation(image, num_colors=6):\n",
    "        # Convert the image to grayscale\n",
    "    grayscale = image.convert('L')\n",
    "    \n",
    "    # Convert grayscale image to numpy array\n",
    "    gray_array = np.array(grayscale)\n",
    "    \n",
    "    # Generate random colors\n",
    "    colors = [colorsys.hsv_to_rgb(np.random.random(), \n",
    "                                  np.random.uniform(0.5, 1.0), \n",
    "                                  np.random.uniform(0.5, 1.0)) for _ in range(num_colors)]\n",
    "    \n",
    "    # Create an empty array for the colored image\n",
    "    colored_array = np.zeros((gray_array.shape[0], gray_array.shape[1], 3), dtype=np.float32)\n",
    "    \n",
    "    # Normalize the gray values to be between 0 and 1\n",
    "    normalized_gray = gray_array / 255.0\n",
    "    \n",
    "    # Apply the color gradient\n",
    "    for i in range(num_colors - 1):\n",
    "        mask = ((normalized_gray >= i / (num_colors - 1)) & \n",
    "                (normalized_gray < (i + 1) / (num_colors - 1)))\n",
    "        \n",
    "        t = (normalized_gray[mask] - i / (num_colors - 1)) * (num_colors - 1)\n",
    "        \n",
    "        colored_array[mask] = (1 - t[:, np.newaxis]) * colors[i] + t[:, np.newaxis] * colors[i+1]\n",
    "    \n",
    "    # Handle the last interval\n",
    "    mask = (normalized_gray >= (num_colors - 1) / (num_colors - 1))\n",
    "    colored_array[mask] = colors[-1]\n",
    "    \n",
    "    # Convert to uint8\n",
    "    colored_array = (colored_array * 255).astype(np.uint8)\n",
    "    \n",
    "    # Convert the numpy array back to a PIL Image\n",
    "    colored_image = Image.fromarray(colored_array)\n",
    "    \n",
    "    return colored_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_image = random_color_augmentation(example_image)\n",
    "# display the original and augmented images\n",
    "augmented_image.save(\"./test/color_random.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diff_aug",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
